{"cells":[{"cell_type":"code","execution_count":null,"id":"d78c72da","metadata":{},"outputs":[],"source":["!gcloud dataproc clusters list --region us-central1"]},{"cell_type":"code","execution_count":1,"id":"7cfb25e4","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\n","\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\n"]}],"source":["!pip install -q google-cloud-storage==1.43.0\n","!pip install -q graphframes"]},{"cell_type":"code","execution_count":2,"id":"f51f385f","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["import pyspark\n","import builtins\n","import sys\n","from collections import Counter, OrderedDict, defaultdict\n","import itertools\n","from itertools import islice, count, groupby\n","import pandas as pd\n","import os\n","import re\n","from operator import itemgetter\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from time import time\n","from pathlib import Path\n","import pickle\n","import pandas as pd\n","from google.cloud import storage\n","from timeit import timeit\n","import random\n","import numpy as np\n","from collections import defaultdict\n","\n","import hashlib\n","def _hash(s):\n","    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n","\n","import ast\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","from nltk.stem import PorterStemmer\n","from contextlib import closing\n","porter = PorterStemmer()\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","stopwords = set(stopwords.words('english')) #stopwords list\n","corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n","                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n","                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n","                    \"many\", \"however\", \"would\", \"became\"]\n","\n","stop = stopwords.union(corpus_stopwords)"]},{"cell_type":"code","execution_count":3,"id":"92dedea3","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["inverted_index_gcp1.py\r\n"]}],"source":["# if nothing prints here you forgot to upload the file inverted_index_gcp.py to the home dir\n","%cd -q /home/dataproc\n","!ls inverted_index_gcp1.py"]},{"cell_type":"code","execution_count":4,"id":"e57efcbb","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["-rw-r--r-- 1 root root 247882 Jan  5 18:18 /usr/lib/spark/jars/graphframes-0.8.2-spark3.1-s_2.12.jar\r\n"]}],"source":["# if nothing prints here you forgot to include the initialization script when starting the cluster\n","!ls -l /usr/lib/spark/jars/graph*"]},{"cell_type":"code","execution_count":5,"id":"22fb47af","metadata":{},"outputs":[],"source":["from pyspark.sql import *\n","from pyspark import SparkContext, SparkConf, SparkFiles\n","from pyspark.sql import SQLContext\n","from graphframes import *"]},{"cell_type":"code","execution_count":6,"id":"62153902","metadata":{},"outputs":[],"source":["# adding our python module to the cluster\n","sc.addFile(\"/home/dataproc/inverted_index_gcp1.py\")\n","sys.path.insert(0,SparkFiles.getRootDirectory())"]},{"cell_type":"code","execution_count":7,"id":"29b17b17","metadata":{},"outputs":[],"source":["from inverted_index_gcp1 import InvertedIndex, MultiFileReader"]},{"cell_type":"code","execution_count":8,"id":"d88ffb15","metadata":{},"outputs":[],"source":["RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){,24}\"\"\", re.UNICODE)\n","\n","def tokenize(text):\n","    token_words = [token.group() for token in RE_WORD.finditer(text)]\n","    stem_sentence=[]\n","    for word in token_words:\n","      if word.lower() not in stop and (word.isalpha() or word.isdigit()):\n","        stem_sentence.append(porter.stem(word.lower()))\n","    return stem_sentence\n","\n","def simple_tokenize(text):\n","    token_words = [token.group() for token in RE_WORD.finditer(text)]\n","    stem_sentence=[]\n","    for word in token_words:\n","      if word not in stop and (word.isalpha() or word.isdigit()):\n","        stem_sentence.append(porter.stem(word.lower()))\n","    return stem_sentence"]},{"cell_type":"code","execution_count":9,"id":"00b6ae04","metadata":{},"outputs":[],"source":["def word_count(id, tokens):\n","  counts = Counter()\n","  postinglist = []\n","  for word in tokens:\n","    counts[word] +=1\n","  for word, count in counts.items():\n","    postinglist.append((word,(id,count)))\n","  return postinglist\n","\n","def reduce_word_counts(unsorted_pl):\n","  return sorted(unsorted_pl, key = lambda x: x[1], reverse = True)\n","\n","def reduce_word_counts1(unsorted_pl):\n","  ps = sorted(unsorted_pl, key = lambda x: len(x[1]), reverse = True)\n","  ps = [(x[0], x[1][:400]) for x in ps]\n","  return ps\n","\n","def calculate_df(postings):\n","  ps = postings.map(lambda x: (x[0],len(x[1])))\n","  return ps\n","\n","NUM_BUCKETS = 124\n","def token2bucket_id(token):\n","  return int(_hash(token),16) % NUM_BUCKETS\n","\n","def partition_postings_and_write(postings, inverted):\n","  ps = postings.map(lambda x: (token2bucket_id(x[0]), [(x[0], x[1])] ))\n","  ps = ps.reduceByKey(lambda accum, n: accum + n ).mapValues(reduce_word_counts1)\n","  ps = ps.map(lambda x: inverted.write_a_posting_list(x , bucket_name))\n","  return ps\n","\n","TUPLE_SIZE = 6       \n","TF_MASK = 2 ** 16 - 1 # Masking the 16 low bits of an integer\n","\n","def read_posting_list(inverted, w, bucket_name):\n","  with closing(MultiFileReader()) as reader:\n","    locs = inverted.posting_locs[w]\n","    b = reader.read(locs, inverted.df[w] * TUPLE_SIZE, bucket_name)\n","    posting_list = []\n","    for i in range(inverted.df[w]):\n","      doc_id = int.from_bytes(b[i*TUPLE_SIZE:i*TUPLE_SIZE+4], 'big')\n","      tf = int.from_bytes(b[i*TUPLE_SIZE+4:(i+1)*TUPLE_SIZE], 'big')\n","      posting_list.append((doc_id, tf))\n","    return posting_list\n","\n","\n","def rare_words(tokens):\n","    t = []\n","    for token in tokens:\n","        if postings_not_filtered[token] != 1:\n","            t.append(token)\n","    return t"]},{"cell_type":"code","execution_count":10,"id":"dfa3b60f","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["full_path = \"gs://wikidata_preprocessed/*\"\n","parquetFile = spark.read.parquet(full_path)\n","doc_text_pairs = parquetFile.select(\"id\", \"text\").rdd\n","tokens_pages = doc_text_pairs.map(lambda x: (x[0], tokenize(x[1])))"]},{"cell_type":"code","execution_count":11,"id":"81f28974","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["graphframes.sh\n","startup_script_gcp.sh\n"]}],"source":["# Put your bucket name below and make sure you can access it without an error\n","bucket_name = 'text_315923151'\n","client = storage.Client()\n","blobs = client.list_blobs(bucket_name)\n","\n","for b in blobs:\n","    print(b.name)\n","\n","NUM_BUCKETS = 248"]},{"cell_type":"code","execution_count":null,"id":"70023ce6","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["22/01/08 17:08:11 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 15.0 in stage 10.0 (TID 813) (cluster-d35d-w-1.us-central1-a.c.top-monitor-337308.internal executor 7): java.net.SocketException: Connection reset\n","\tat java.net.SocketInputStream.read(SocketInputStream.java:210)\n","\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\n","\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n","\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\n","\tat java.io.DataInputStream.readInt(DataInputStream.java:387)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:642)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n","\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n","\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n","\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n","\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n","\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n","\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n","\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n","\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n","\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n","\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n","\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2244)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:748)\n","\n","22/01/08 17:08:12 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 10.0 in stage 10.0 (TID 814) (cluster-d35d-w-3.us-central1-a.c.top-monitor-337308.internal executor 26): java.net.SocketException: Broken pipe (Write failed)\n","\tat java.net.SocketOutputStream.socketWrite0(Native Method)\n","\tat java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111)\n","\tat java.net.SocketOutputStream.write(SocketOutputStream.java:155)\n","\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\n","\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n","\tat java.io.FilterOutputStream.write(FilterOutputStream.java:97)\n","\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:295)\n","\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:307)\n","\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:307)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n","\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:621)\n","\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:397)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\n","\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:232)\n","\n","22/01/08 17:08:36 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 32.0 in stage 10.0 (TID 827) (cluster-d35d-w-1.us-central1-a.c.top-monitor-337308.internal executor 21): java.net.SocketException: Broken pipe (Write failed)\n","\tat java.net.SocketOutputStream.socketWrite0(Native Method)\n","\tat java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111)\n","\tat java.net.SocketOutputStream.write(SocketOutputStream.java:155)\n","\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\n","\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n","\tat java.io.FilterOutputStream.write(FilterOutputStream.java:97)\n","\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:295)\n","\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:307)\n","\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:307)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n","\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:621)\n","\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:397)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\n","\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:232)\n","\n","22/01/08 17:08:56 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 37.0 in stage 10.0 (TID 837) (cluster-d35d-w-1.us-central1-a.c.top-monitor-337308.internal executor 21): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:550)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:539)\n","\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:657)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n","\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n","\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n","\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n","\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n","\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n","\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n","\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n","\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n","\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n","\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n","\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2244)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:748)\n","Caused by: java.io.EOFException\n","\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:642)\n","\t... 29 more\n","\n","22/01/08 17:09:05 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 43.0 in stage 10.0 (TID 850) (cluster-d35d-w-0.us-central1-a.c.top-monitor-337308.internal executor 24): java.net.SocketException: Broken pipe (Write failed)\n","\tat java.net.SocketOutputStream.socketWrite0(Native Method)\n","\tat java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111)\n","\tat java.net.SocketOutputStream.write(SocketOutputStream.java:155)\n","\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\n","\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n","\tat java.io.FilterOutputStream.write(FilterOutputStream.java:97)\n","\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:295)\n","\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:307)\n","\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:307)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n","\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:621)\n","\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:397)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\n","\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:232)\n","\n","22/01/08 17:09:17 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 57.0 in stage 10.0 (TID 858) (cluster-d35d-w-1.us-central1-a.c.top-monitor-337308.internal executor 7): java.net.SocketException: Broken pipe (Write failed)\n","\tat java.net.SocketOutputStream.socketWrite0(Native Method)\n","\tat java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111)\n","\tat java.net.SocketOutputStream.write(SocketOutputStream.java:155)\n","\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\n","\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n","\tat java.io.FilterOutputStream.write(FilterOutputStream.java:97)\n","\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:295)\n","\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:307)\n","\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:307)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n","\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:621)\n","\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:397)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\n","\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:232)\n","\n","22/01/08 17:09:29 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 63.0 in stage 10.0 (TID 867) (cluster-d35d-w-0.us-central1-a.c.top-monitor-337308.internal executor 24): java.net.SocketException: Connection reset\n","\tat java.net.SocketInputStream.read(SocketInputStream.java:210)\n","\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\n","\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n","\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\n","\tat java.io.DataInputStream.readInt(DataInputStream.java:387)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:642)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n","\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n","\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n","\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n","\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n","\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n","\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n","\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n","\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n","\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n","\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n","\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2244)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:748)\n","\n","22/01/08 17:09:42 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 76.0 in stage 10.0 (TID 876) (cluster-d35d-w-1.us-central1-a.c.top-monitor-337308.internal executor 7): java.net.SocketException: Connection reset\n","\tat java.net.SocketInputStream.read(SocketInputStream.java:210)\n","\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\n","\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n","\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\n","\tat java.io.DataInputStream.readInt(DataInputStream.java:387)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:642)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n","\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n","\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n","\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n","\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n","\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n","\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n","\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n","\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n","\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n","\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n","\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2244)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:748)\n","\n","22/01/08 17:10:03 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 100.0 in stage 10.0 (TID 889) (cluster-d35d-w-1.us-central1-a.c.top-monitor-337308.internal executor 21): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:550)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:539)\n","\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:657)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n","\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n","\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n","\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n","\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n","\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n","\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n","\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n","\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n","\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n","\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n","\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2244)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:748)\n","Caused by: java.io.EOFException\n","\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:642)\n","\t... 29 more\n","\n","22/01/08 17:10:26 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 123.0 in stage 10.0 (TID 908) (cluster-d35d-w-1.us-central1-a.c.top-monitor-337308.internal executor 21): java.net.SocketException: Connection reset\n","\tat java.net.SocketInputStream.read(SocketInputStream.java:210)\n","\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\n","\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n","\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\n","\tat java.io.DataInputStream.readInt(DataInputStream.java:387)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:642)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n","\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n","\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n","\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n","\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n","\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n","\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n","\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n","\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n","\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n","\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n","\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2244)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:748)\n","\n","22/01/08 17:10:42 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 39.0 in stage 10.0 (TID 915) (cluster-d35d-w-1.us-central1-a.c.top-monitor-337308.internal executor 21): java.net.SocketException: Broken pipe (Write failed)\n","\tat java.net.SocketOutputStream.socketWrite0(Native Method)\n","\tat java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111)\n","\tat java.net.SocketOutputStream.write(SocketOutputStream.java:155)\n","\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\n","\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n","\tat java.io.FilterOutputStream.write(FilterOutputStream.java:97)\n","\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:295)\n","\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:307)\n","\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:307)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n","\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:621)\n","\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:397)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\n","\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:232)\n","\n","22/01/08 17:11:05 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 91.0 in stage 10.0 (TID 935) (cluster-d35d-w-1.us-central1-a.c.top-monitor-337308.internal executor 7): java.net.SocketException: Broken pipe (Write failed)\n","\tat java.net.SocketOutputStream.socketWrite0(Native Method)\n","\tat java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111)\n","\tat java.net.SocketOutputStream.write(SocketOutputStream.java:155)\n","\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\n","\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n","\tat java.io.FilterOutputStream.write(FilterOutputStream.java:97)\n","\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:295)\n","\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:307)\n","\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:307)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n","\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:621)\n","\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:397)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\n","\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:232)\n","\n","                                                                                \r"]}],"source":["text_InvertedIndex = InvertedIndex()\n","\n","# for each word we save how many times each word appear in each doc\n","word_counts = tokens_pages.flatMap(lambda x: word_count(x[0], x[1]))\n","\n","# tuple of word and posting list(each posting list sort by doc id)\n","postings = word_counts.groupByKey()\n","\n","# we will filter out rare words, words that appear in 50 or fewer documents.\n","postings_filtered = postings.filter(lambda x: len(x[1]) > 50).mapValues(reduce_word_counts)\n","\n","# len of each document\n","text_InvertedIndex.len_docs = Counter(tokens_pages.map(lambda x: (x[0], len(x[1]))).collectAsMap())\n","\n","# number of documents\n","text_InvertedIndex.number_of_docs = len(text_InvertedIndex.len_docs)\n","\n","# doc frequency for each term\n","text_InvertedIndex.df = Counter(calculate_df(postings_filtered).collectAsMap())\n","\n","# posting list of td-idf for each word(top 400 sort by tf)\n","tf_idf_filtered = postings_filtered.map(lambda x: (x[0],[(y[0],(y[1]/text_InvertedIndex.len_docs[y[0]]) * np.log2(text_InvertedIndex.number_of_docs/text_InvertedIndex.df[x[0]])) for y in x[1]]))\n","\n","# mehane for each doc_id\n","text_InvertedIndex.sum_idf = tf_idf_filtered.flatMap(lambda x: [(y[0],y[1]**2)  for y in x[1]]).reduceByKey(lambda x, y: x+y).collectAsMap()\n","\n","# posting locs\n","posting_locs_list = partition_postings_and_write(postings_filtered, text_InvertedIndex).collect()"]},{"cell_type":"code","execution_count":13,"id":"c1a4d91c","metadata":{},"outputs":[],"source":["# collect all posting lists locations into one super-set\n","super_posting_locs_text = defaultdict(list)\n","for blob in client.list_blobs(bucket_name, prefix='postings_gcp'):\n","  if not blob.name.endswith(\"pickle\"):\n","    continue\n","  with blob.open(\"rb\") as f:\n","    posting_locs = pickle.load(f)\n","    for k, v in posting_locs.items():\n","      super_posting_locs_text[k].extend(v)\n","text_InvertedIndex.posting_locs = super_posting_locs_text"]},{"cell_type":"code","execution_count":14,"id":"20f333f5","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Copying file://text_InvertedIndex.pkl [Content-Type=application/octet-stream]...\n","==> NOTE: You are uploading one or more large file(s), which would run          \n","significantly faster if you enable parallel composite uploads. This\n","feature can be enabled by editing the\n","\"parallel_composite_upload_threshold\" value in your .boto\n","configuration file. However, note that if you do this large files will\n","be uploaded as `composite objects\n","<https://cloud.google.com/storage/docs/composite-objects>`_,which\n","means that any user who downloads such objects will need to have a\n","compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n","without a compiled crcmod, computing checksums on composite objects is\n","so slow that gsutil disables downloads of composite objects.\n","\n","| [1 files][238.4 MiB/238.4 MiB]                                                \n","Operation completed over 1 objects/238.4 MiB.                                    \n","238.42 MiB  2022-01-08T17:12:06Z  gs://text_315923151/postings_gcp/text_InvertedIndex.pkl\n","TOTAL: 1 objects, 249999986 bytes (238.42 MiB)\n"]}],"source":["# write the global stats out\n","text_InvertedIndex.write_index('.', 'text_InvertedIndex')\n","# upload to gs\n","index_src = \"text_InvertedIndex.pkl\"\n","index_dst = f'gs://{bucket_name}/postings_gcp/{index_src}'\n","!gsutil cp $index_src $index_dst\n","!gsutil ls -lh $index_dst"]},{"cell_type":"code","execution_count":null,"id":"aa213166","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"}},"nbformat":4,"nbformat_minor":5}